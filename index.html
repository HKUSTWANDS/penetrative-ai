<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/html">
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Penetrative AI.">
  <meta name="keywords" content="Penetrative AI">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Penetrative AI</title>


  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css" integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">
  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">

  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/wands.png">



</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Penetrative AI: Making LLMs Comprehend the Physical World</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://dapowan.github.io/">Huatao Xu</a><sup>†</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.linkedin.com/in/liying-han-b7b60325b">Liying Han</a><sup>§</sup>,
            </span>
            <span class="author-block">
              <a href="">Qirui Yang</a><sup>*</sup>,
            </span>
            <span class="author-block">
              <a href="https://cse.hkust.edu.hk/~lim/">Mo Li</a><sup>*,†</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.ee.ucla.edu/mani-srivastava/">Mani Srivastava</a><sup>§</sup>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>*</sup>The Hong Kong University of Science and Technology,</span><br>
            <span class="author-block"><sup>†</sup>Nanyang Technological University,</span>
            <span class="author-block"><sup>§</sup>University of California Los Angeles</span>

          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://dapowan.github.io/files/PenAI.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2310.09605"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
<!--               Slides Link.-->
<!--              <span class="link-block">-->
<!--                <a href=""-->
<!--                   class="external-link button is-normal is-rounded is-dark">-->
<!--                  <span class="icon">-->
<!--                      <i class="fab fa fa-file-image"></i>-->
<!--                  </span>-->
<!--                  <span>Slides</span>-->
<!--                </a>-->
<!--              </span>-->
              <!-- Video Link. -->
<!--              <span class="link-block">-->
<!--                <a href=""-->
<!--                   class="external-link button is-normal is-rounded is-dark">-->
<!--                  <span class="icon">-->
<!--                      <i class="fab fa-youtube"></i>-->
<!--                  </span>-->
<!--                  <span>Video</span>-->
<!--                </a>-->
<!--              </span>-->
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/dapowan/Penetrative-AI"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
<!--              <span class="link-block">-->
<!--                <a href=""-->
<!--                   class="external-link button is-normal is-rounded is-dark">-->
<!--                  <span class="icon">-->
<!--                      <i class="far fa-images"></i>-->
<!--                  </span>-->
<!--                  <span>Data</span>-->
<!--                  </a>-->
<!--              </span>-->
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="row justify-content-center">
      <div class="content col-sm-12 col-md-10 col-lg-6">
        <!-- Your content goes here -->
        <img src="./static/images/objective.jpg"
             class="image-scenario"
             alt="Interpolate start reference image."/>

        <h4 class="subtitle has-text-centered">
          Figure 1. Overview of Penetrative AI.
        </h4>
      </div>
    </div>
  </div>

</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Recent developments in Large Language Models (LLMs) have demonstrated their remarkable capabilities across a range of tasks.
            Questions, however, persist about the nature of LLMs and their potential to integrate common-sense human knowledge when performing tasks involving information about the real physical world.
          </p>
          <p>
            This paper delves into these questions by exploring how LLMs can be extended to interact with and reason about the physical world through IoT sensors and actuators, a concept that we term <strong>Penetrative AI</strong>.
            The paper explores such an extension at two levels of LLMs' ability to penetrate into the physical world via the processing of sensory signals.
          </p>
          <p>
            Our preliminary findings indicate that LLMs, with ChatGPT being the representative example in our exploration, have considerable and unique proficiency in employing the knowledge they learned during training for interpreting IoT sensor data and reasoning over them about tasks in the physical realm.
          </p>
          <p>
            Not only this opens up new applications for LLMs beyond traditional text-based tasks, but also enables new ways of incorporating human knowledge in cyber-physical systems.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. -->
<!--    <div class="columns is-centered has-text-centered">-->
<!--      <div class="column is-four-fifths">-->
<!--        <h2 class="title is-3">Video</h2>-->
<!--        <div class="publication-video">-->
<!--          <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0"-->
<!--                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>-->
<!--        </div>-->
<!--      </div>-->
<!--    </div>-->
    <!--/ Paper video. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">

<!--     Overview.-->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Introduction</h2>
<!--        <br/>-->
        <!--/ Interpolating. -->

        <!-- Re-rendering. -->
<!--        <h3 class="title is-4">Re-rendering the input video</h3>-->
<!--        <div class="row justify-content-center">-->
<!--          <div class="content col-sm-12 col-md-10 col-lg-8">-->
<!--            <img src="./static/images/overview_concise.jpg"-->
<!--                 class="image-scenario"-->
<!--                 alt="Interpolate start reference image."/>-->
<!--            <h4 class="subtitle has-text-centered">UniHAR overview.</h4>-->
<!--          </div>-->
<!--        </div>-->
        <div class="content has-text-justified">
          <p>
            Large Language Models (LLMs) cultivated on extensive text datasets have showcased remarkable capabilities across diverse tasks, including coding and logical problem-solving.
            These out-of-the-box capabilities have demonstrated that they already comprise enormous amounts of common human knowledge (Some studies referred to it as a world model of how the world works).
          </p>
          <p>
            This paper is motivated by an essential and intriguing question: can we enable LLMs to complete tasks in the real physical world? We delve into this inquiry and explore extending the boundaries of LLM capabilities by directly letting them interact with the physical world through Internet of Things (IoT) sensors.
            A basic example of this process is depicted in Figure 1, where different from the conventional way of LLM usage in natural language tasks, an LLM is expected to analyze sensor data and aided by expert knowledge to deduce the physical states of the object.
            These sensor and actuator readings are indeed projections from the physical world, and the LLM is expected to harness its common human knowledge to comprehend sensor data and execute perception tasks.
          </p>
          <p>
            We formulate such a problem from a signal processing's point of view, and specifically explore the LLMs' penetration into the physical world at two signal processing levels of the sensor data:
          </p>
          <ul>
            <li>
              <p>
                <strong>Textualized-level penetration. </strong>
                LLMs are instructed to process textualized signal derived from underlying sensor data.
              </p>
            </li>
            <li>
              <p>
                <strong>Digitized-level penetration. </strong>
                LLMs are guided to digitized signal, essentially numerical sequences of raw sensor data.
              </p>
            </li>
          </ul>
          <p>
            We term this endeavor "<strong>Penetrative AI</strong>" -- where the embedded world knowledge in large language models (LLMs) serves as a foundation, seamlessly integrated with the Cyber-Physical Systems (CPS) for perceiving and intervening in the physical world.
          </p>
          <p>
            Our methodology is exemplified through two illustrative applications at two different levels, respectively. Next, we will elaborate on the design and experiment results of these two illustrative applications.
          </p>
        </div>

        <!--/ Re-rendering. -->

      </div>
    </div>
<!--    / Overview.-->

    <!-- Activity Sensing. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Activity Sensing</h2>
<!--        <br/>-->
        <!--/ Interpolating. -->

        <!-- Re-rendering. -->
<!--        <h3 class="title is-4">Re-rendering the input video</h3>-->
        <div class="content has-text-justified">
          <p>
            This section describes our effort in tasking ChatGPT, a chosen vehicle, to comprehend IoT sensor data at the textualized signal level. </p>
        </div>
        <h3 class="title is-4">Design</h3>
        <div class="content has-text-justified">
          <p>
            We take activity sensing as an illustrative example, where we task ChatGPT with the interpretation of sensor data collected from smartphones to derive user activities. The input sensor data encompass smartphone accelerometer, satellite, and WiFi signals, and the desired output is to discern the user motion and environment context.
          </p>
        </div>
        <div class="row justify-content-center">
          <div class="content col-sm-12 col-md-10 col-lg-10">
            <img src="./static/images/as_full_hotmobile.jpg"
                 class="image-scenario"
                 alt="Interpolate start reference image."/>
            <h4 class="subtitle has-text-centered">Figure 2. Overview of activity sensing with LLMs.</h4>
          </div>
        </div>
        <div class="content has-text-justified">
          <p>
            To facilitate ChatGPT comprehension of the sensor data, we undertake a preprocessing step where raw data from different sensing modules are separately converted into textualized states that are expected interpretable by ChatGPT.
            A full prompt includes a defined objective and expert knowledge of the sensor data, all in natural language. Essentially, the way we construct the prompt serves as a means to educate and instruct ChatGPT to interpret sensor data.
          </p>
        </div>
        <h3 class="title is-4">Preliminary Results</h3>
        <div class="row justify-content-center">
          <div class="content col-sm-12 col-md-12 col-lg-12">
            <img src="./static/images/as_exp.jpg"
                 class="image-scenario"
                 alt="Interpolate start reference image."/>
            <h4 class="subtitle has-text-centered">Figure 3. Response examples of ChatGPT-4 for activity sensing.</h4>
          </div>
        </div>
        <div class="content has-text-justified">
          <p>
            Our preliminary results suggest LLMs are highly effective in analyzing physical world signals when they are properly abstracted into textual representations.
            These findings align with our initial expectations based on the "world model" hypothesis of LLMs.
          </p>
          <p>
            <i class="fas fa-hand-point-right"></i> Check more examples and our prompt at <a href="https://github.com/dapowan/Penetrative-AI/blob/main/demo/activity_sensing.md" target="_blank"><i class="fab fa-github"></i>here</a>.
          </p>
        </div>
        <!--/ Re-rendering. -->

      </div>
    </div>
    <!--/ Activity Sensing. -->

    <!-- Heartbeat Detection. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Heartbeat Detection</h2>
        <!--        <br/>-->
        <!--/ Interpolating. -->

        <!-- Re-rendering. -->
        <!--        <h3 class="title is-4">Re-rendering the input video</h3>-->
        <div class="content has-text-justified">
          <p>
            This section describes our effort to go beyond the general expectations of the textualized signal processing ability of LLMs. We specifically study the potential of ChatGPT in comprehending digitized sensor signals.
          </p>
        </div>
        <h3 class="title is-4">Design</h3>
        <div class="content has-text-justified">
          <p>
            We take human heartbeat detection as an illustrative example, where we task ChatGPT with the input of ECG waveforms to derive the heartbeat rate. Fundamentally different from the previous example, all sensor data in this application are expressed as sequences of digitized samples.
            Figure 4 provides an overview of the design.</p>
        </div>
        <div class="row justify-content-center">
          <div class="content col-sm-12 col-md-10 col-lg-10">
            <img src="./static/images/hd_full.jpg"
                 class="image-scenario"
                 alt="Interpolate start reference image."/>
            <h4 class="subtitle has-text-centered">Figure 4. Overview of heartbeat detection with LLMs.</h4>
          </div>
        </div>
        <div class="content has-text-justified">
          <p>
            Our prior experiments showed that ChatGPT failed to identify most R-peaks without guidance. However, R-peak selection seems to be an easy task for human.
            So how do we identify R-peaks? Based on manual observations, we design a natural language-based "algorithm" that LLMs understand to guide the selection of R-peaks:
          </p>
          <ol>
            <li>
              <p>
                <strong>Initial Observation. </strong>
                Begin by observing the rough overall range of ECG numbers in the provided data.
              </p>
            </li>
            <li>
              <p>
                <strong>Identify Subsequences. </strong>
                Find subsequences of numbers that meet the following criteria:
                <ul>
              <li>The initial numbers are in the lower part of the overall range, even smaller than the range.</li>
              <li>Subsequent numbers exhibit a significant increase, even exceeding the overall range.</li>
              <li>Following the increase, subsequent numbers return to the overall range.</li>
            </ul>
              </p>
            </li>
            <li>
              <p>
                <strong>Select R-Peaks. </strong>
                After identifying these subsequences, select the largest number from each subsequence as an R-peak.
              </p>
            </li>
          </ol>
          <p>
            We investigate whether ChatGPT can effectively execute such fuzzy logic (without explicit threshold values) when processing the digitized signals.
            We call this algorithm the <strong>'natural language algorithm'</strong> as it is expressed as text.
          </p>
        </div>
        <h3 class="title is-4">Preliminary Results</h3>
        <div class="row justify-content-center">
          <div class="content col-sm-12 col-md-10 col-lg-10">
            <img src="./static/images/hd_exp.jpg"
                 class="image-scenario"
                 alt="Interpolate start reference image."/>
            <h4 class="subtitle has-text-centered">Figure 5. Response examples of ChatGPT-4 for heartbeat detection.</h4>
          </div>
        </div>
        <div class="content has-text-justified">
          <p>
            Our initial findings indicate that LLMs, particularly ChatGPT-4, exhibit remarkable proficiency in analyzing physical digitized signals when provided with proper guidance.
            These results provide additional evidence supporting the "world model" hypothesis of LLMs.
          </p>
          <p>
            <i class="fas fa-hand-point-right"></i> Check more examples and our prompt at <a href="https://github.com/dapowan/Penetrative-AI/blob/main/demo/heartbeat_detection.md" target="_blank"><i class="fab fa-github"></i>here</a>.
          </p>
        </div>

      </div>
    </div>
    <!--/ Heartbeat Detection. -->

    <!-- Adoption. -->
<!--    <div class="columns is-centered">-->
<!--      <div class="column is-full-width">-->
<!--        <h2 class="title is-3">UniHAR Adoption</h2>-->
<!--&lt;!&ndash;        <br/>&ndash;&gt;-->
<!--        &lt;!&ndash;/ Interpolating. &ndash;&gt;-->

<!--        <div class="content has-text-justified">-->
<!--          <p>-->
<!--            In practical applications, UniHAR is a configurable framework that can adapt to two scenarios, i.e., <i>data-decentralized</i> and <i>data-centralized</i> scenarios.-->
<!--          </p>-->
<!--          <p>-->
<!--            In the data-decentralized scenario where raw data transmission is not encouraged, UniHAR collaborates with all users and integrates self-supervised and federated learning techniques to train a generalized feature extraction model using massive and augmented unlabeled data.-->
<!--            UniHAR then constructs an activity recognition model using limited but augmented labeled data from source users.-->
<!--          </p>-->
<!--          <p>-->
<!--            In the data-centralized scenario, where raw data transmissions from target users are possible, UniHAR can further leverage adversarial training techniques for improved performance.-->
<!--          </p>-->

<!--        </div>-->

<!--        &lt;!&ndash;/ Re-rendering. &ndash;&gt;-->

<!--      </div>-->
<!--    </div>-->
    <!--/ Adoption. -->

    <!-- Concurrent Work. -->
<!--    <div class="columns is-centered">-->
<!--      <div class="column is-full-width">-->
<!--        <h2 class="title is-3">Related Links</h2>-->

<!--        <div class="content has-text-justified">-->
<!--          <p>-->
<!--            <i class="fas fa-hand-point-right"></i> Check <a href="https://dapowan.github.io/files/UniHAR.pdf" target="_blank"><i class="fas fa-file-pdf"></i>UniHAR</a> paper for more details.-->
<!--          </p>-->
<!--          <p>-->
<!--            <i class="fas fa-hand-point-right"></i> Check <a href=""><i class="fab fa-github"></i>UniHAR</a>-->
<!--            if you are interested in implementing UniHAR with Pytorch.-->
<!--          </p>-->
<!--          <p>-->
<!--            <i class="fas fa-hand-point-right"></i> Check <a href=""><i class="fab fa-github"></i>UniHAR Mobile</a>-->
<!--            if you are interested in building HAR models on Android devices with Tensorflow lite.-->
<!--          </p>-->
<!--          <p>-->
<!--            <i class="fas fa-hand-point-right"></i> Check our previous work <a href="https://github.com/dapowan/LIMU-BERT-Public" target="_blank"><i class="fab fa-github"></i>LIMU-BERT</a> (an <strong>IMU foundation model</strong>) if you are interested in self-supervised representation learning for IMU data.-->
<!--          </p>-->

<!--        </div>-->
<!--      </div>-->
<!--    </div>-->
    <!--/ Concurrent Work. -->

    <!-- Prompt. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Prompt</h2>

        <div class="content has-text-justified">
          <p>
            <i class="fas fa-hand-point-right"></i> Check <a href="https://dapowan.github.io/files/PenAI.pdf" target="_blank"><i class="fas fa-file-pdf"></i>PenAI</a> paper for more details.
          </p>
          <p>
            <i class="fas fa-hand-point-right"></i> Try our <a href="https://github.com/dapowan/Penetrative-AI" target="_blank"><i class="fas fa-file-code"></i>prompt</a>.
          </p>

        </div>
      </div>
    </div>
    <!--/ Prompt. -->
  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@misc{xu2024penetrative,
      title={Penetrative AI: Making LLMs Comprehend the Physical World}, 
      author={Huatao Xu and Liying Han and Qirui Yang and Mo Li and Mani Srivastava},
      year={2024},
      eprint={2310.09605},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            Website template borrowed from <a href="https://github.com/nerfies/nerfies.github.io">NeRFies</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

<!--  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>-->
<script defer src="./static/js/fontawesome.all.min.js"></script>
<!--  <script src="./static/js/jquery.min.js">  </script>-->
<script src="https://code.jquery.com/jquery-3.2.1.slim.min.js" integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/popper.js@1.12.9/dist/umd/popper.min.js" integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/js/bootstrap.min.js" integrity="sha384-JZR6Spejh4U02d8jOt6vLEHfe/JQGiRRSQQxSfFWpi1MquVdAyjUar5+76PVCmYl" crossorigin="anonymous"></script>
<script src="./static/js/bulma-carousel.min.js"></script>
<script src="./static/js/bulma-slider.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.8/clipboard.min.js"></script>
<script src="./static/js/index.js"></script>
</body>
</html>
